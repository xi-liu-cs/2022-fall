\documentclass[12pt, border = 4pt, multi]{article} % \documentclass[tikz, border = 4pt, multi]{article}
\usepackage[a4paper, margin = 70pt]{geometry}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{amssymb} % mathbb{}
\usepackage[dvipsnames]{xcolor}
\usepackage{forest}
\usepackage[pdftex]{hyperref}
\usepackage{amsmath} % matrices
\usepackage{xeCJK}
\usepackage{tikz}
\usepackage[arrowdel]{physics}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage{pgfplots, pgfplotstable}
\usepackage{diagbox} % diagonal line in cell
\usepackage[usestackEOL]{stackengine}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[T1]{fontenc} 
\setlength{\columnsep}{1cm}
\graphicspath{{./img}} % specify the graphics path to be relative to the main .tex file, denoting the main .tex file directory as ./
\definecolor{orchid}{rgb}{0.7, 0.4, 1.1}
\lstset
{ 
  backgroundcolor = \color{white},
  basicstyle = \scriptsize,
  breaklines = true,
  commentstyle = \color{comment_color}\textit,
  keywordstyle = \color{keyword_color}\bfseries,
  language = c++,
  escapeinside = {\%*}{*)},          
  extendedchars = true,              
  frame = tb,
  numberstyle = \tiny\color{comment_color},
  rulecolor = \color{black},
  showstringspaces = false,
  stringstyle = \color{string_color},
  upquote = true, 
}
\usepackage{xcolor}
\definecolor{comment_color}{rgb}{0, 0.5, 0}
\definecolor{keyword_color}{rgb}{0.3, 0, 0.6}
\definecolor{string_color}{rgb}{0.5, 0, 0.1}
\begin{document}
\section*{Xi Liu}
methodology:\\
libraries: 
\begin{verbatim}
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
\end{verbatim}\\
details: use read\_csv to store data into a pandas data frame df. last column in spambase.data is the label (y) for training the model, read last column from df. before train the model, remove label from data, remove last column from df. transfer df to array. split dataset into train and test dataset paired with its label. set up kernel and hyper-parameter (C, gamma) for the model. train the model. inputs are train dataset and train label. inputs are test dataset and test label (see code).\\
\\
experimental results:\\
table can be seen at following page\\
\\
intuition for results observed for different kernels and C:\\
linear: $<x, x'>$\\
polynomial: $(\gamma<x, x'> + r) ^ d$, $d$ is specified by degree\\
radial basis function: $\exp(-\gamma||x - x'|| ^ 2)$\\
soft margin SVM objective function:
\begin{align*}
\frac{1}{2}\sum_{k = 1} ^ n w_k ^ 2 &+ C\sum_{i = 1} ^ l \xi_i \rightarrow \min_{w, b, \xi}\\
&\text{s.t.}\\
y_i\left(\sum_{k = 1} ^ n w_k x_{ik} + b\right) &\geq 1 - \xi_i, i = 1, 2, ..., l\\
\xi_i \geq 0, i = 1, 2, ..., l
\end{align*}
parameter C trades off between goals of maximizing margin and minimizing errors. when C is small, sum of error terms become small in objective function, so optimization goal is maximize margin, makes the decision surface smooth, as a result, margin can become so large that includes all points. when C is large, sum of error terms become large in objective function, so optimization goal is minimize sum of error terms, it aims at classifying all training examples correctly. as a result, margin can become so small that include no points.\\
large $C$ has lower bias and high variance, small $C$ has higher bias and low variance\\
\\
for this svm:\\
$O(\text{linear}) = O(\text{quadratic}) = O(\text{radial basis function})$, for kernels that have higher asymptotic complexity, more regularization is needed to avoid overfit (good performance on training data, poor generalization to test data), since $C$ is inversely proportional to regularization strength, for high test accuracy, radial basis function kernel's $C$ < quadratic function kernel's $C$ < linear function kernel's $C$\\
so for this svm, radial basis function has high test accuracy when $C$ is $1 = 10 ^ {-0}$, quadratic kernel has high test accuracy when $C$ is $10 ^ 1$, linear kernel has high test accuracy when $C$ is $10 ^ 4$\\
\end{document}
