entropy
H(Y) = -sum_{i = 1} ^ k P(Y = y_i)log_2(P(Y = y_i))

P(Y = heads) = P(Y = tails) = 0.5
H(Y) =  -sum_{i = 1} ^ k P(Y = y_i)log_2(P(Y = y_i))
= -(0.5 log_2(0.5) + 0.5 log_2(0.5))
= -(-0.5 - 0.5)
= 1 bit
expected number of bits to represent this value
high entropy: Y is uniform/histogram is flat/less predictable
low entropy: Y has peaks and valleys/more likely that events with peak probability occur/more predictable

H(Y | X) = -sum_{j = 1} (P(X = x_j) H(Y | X = x_j))
-sum_{j = 1} ^ k (P(X = x_j) sum_{i = 1} ^ k P(Y = y_i | X = x_i) log_2 P(Y = y_i | X = x_i))

information gain
IG(X) = H(Y) - H(Y | X)
